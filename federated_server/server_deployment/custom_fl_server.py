# 3_server_deployment/custom_fl_server.py (FINAL, UNIFIED, MOMENTUM-COMPATIBLE VERSION)

import numpy as np
from flask import Flask, request, jsonify, send_from_directory
from waitress import serve
from pathlib import Path
import base64
import time
import threading
import json

# =================================================================================
#                            EXPERIMENT HYPERPARAMETERS
# =================================================================================
HOST = "0.0.0.0"
PORT = 8080
NUM_EXPECTED_CLIENTS = 1
NUM_ROUNDS = 3
LOCAL_EPOCHS = 1
# =================================================================================

# --- Asset Paths ---
SCRIPT_DIR = Path(__file__).parent
INITIAL_WEIGHTS_PATH = SCRIPT_DIR / "initial_weights.bin"
# --- NEW: Paths for serving model updates to the Momentum app's Settings screen ---
# These files should be manually placed here after being generated by export.py
DEPLOYMENT_ASSETS_DIR = SCRIPT_DIR / "deployment_assets"
DEPLOYMENT_ASSETS_DIR.mkdir(exist_ok=True)

# --- Server State (Thread-Safe) ---
SERVER_STATE_LOCK = threading.Lock()
SERVER_STATE = {
    "global_model_weights": None,
    "initial_weights": None,
    "current_round": 1,
    "server_status": "INITIALIZING",
    "clients_in_round": {},
    "client_updates_in_round": [],
}

app = Flask(__name__)

def load_initial_weights(is_reset=False):
    # ... (This function is unchanged) ...
    with SERVER_STATE_LOCK:
        action = "Resetting" if is_reset else "Loading initial"
        print(f"--- {action} model state from binary file ---")
        try:
            if SERVER_STATE["initial_weights"] is None:
                print(f"Loading from disk: {INITIAL_WEIGHTS_PATH}")
                if not INITIAL_WEIGHTS_PATH.exists():
                    raise FileNotFoundError(f"'{INITIAL_WEIGHTS_PATH.name}' not found.")
                weight_bytes = INITIAL_WEIGHTS_PATH.read_bytes()
                SERVER_STATE["initial_weights"] = np.frombuffer(weight_bytes, dtype=np.float32)

            SERVER_STATE["global_model_weights"] = SERVER_STATE["initial_weights"]
            SERVER_STATE["current_round"] = 1
            SERVER_STATE["clients_in_round"].clear()
            SERVER_STATE["client_updates_in_round"].clear()
            SERVER_STATE["server_status"] = "WAITING_FOR_CLIENTS"
            
            print(f"âœ… State reset. Array shape: {SERVER_STATE['global_model_weights'].shape}")
            print(f"Server is now WAITING for {NUM_EXPECTED_CLIENTS} client(s) to join Round 1.")
            return True
        except Exception as e:
            print(f"âŒ CRITICAL ERROR: Could not load initial state. {e}")
            SERVER_STATE["server_status"] = "ERROR"
            return False

# =================================================================================
#                 NEW: MODEL MANAGEMENT ENDPOINTS (from central_server.py)
# =================================================================================
@app.route('/model_info', methods=['GET'])
def get_model_info():
    """Serves model metadata to the app's Settings screen."""
    try:
        return send_from_directory(DEPLOYMENT_ASSETS_DIR, 'model_info.json')
    except FileNotFoundError:
        return jsonify({"error": "model_info.json not found in deployment_assets"}), 404

@app.route('/download_model', methods=['GET'])
def download_model():
    """Serves the inference TFLite model to the app's Settings screen."""
    try:
        # Note: The app expects 'custom_inference_model.tflite' but downloads to that name.
        # We serve a generic file name from the server.
        return send_from_directory(DEPLOYMENT_ASSETS_DIR, 'inference_model.tflite')
    except FileNotFoundError:
        return jsonify({"error": "inference_model.tflite not found in deployment_assets"}), 404

@app.route('/download_scaler_for_app', methods=['GET'])
def download_scaler_for_app():
    """Serves the scaler to the app's Settings screen."""
    try:
        return send_from_directory(DEPLOYMENT_ASSETS_DIR, 'scaler.json')
    except FileNotFoundError:
        return jsonify({"error": "scaler.json not found in deployment_assets"}), 404

# =================================================================================
#                   CUSTOM FEDERATED LEARNING ENDPOINTS
# =================================================================================
# --- These routes are unchanged from your working custom server ---
@app.route('/client-check-in', methods=['POST'])
def client_check_in():
    # ... (This function is unchanged) ...
    with SERVER_STATE_LOCK:
        client_id = request.json.get('client_id')
        status = SERVER_STATE["server_status"]
        current_round = SERVER_STATE["current_round"]
        
        print(f"[{time.ctime()}] Client '{client_id}' checking in. Server status: {status}, Round: {current_round}")

        if status == "AGGREGATING" and client_id in SERVER_STATE["clients_in_round"]:
            return jsonify({"action": "WAIT"})
            
        if current_round > NUM_ROUNDS or status == "COMPLETE":
            return jsonify({"action": "COMPLETE"})

        if status == "WAITING_FOR_CLIENTS":
            if client_id not in SERVER_STATE["clients_in_round"]:
                SERVER_STATE["clients_in_round"][client_id] = "joined"
                print(f" > Client '{client_id}' has joined Round {current_round}.")
        
        if len(SERVER_STATE["clients_in_round"]) >= NUM_EXPECTED_CLIENTS:
            weights_bytes = SERVER_STATE["global_model_weights"].tobytes()
            weights_b64 = base64.b64encode(weights_bytes).decode('utf-8')
            training_config = {"local_epochs": LOCAL_EPOCHS}
            
            return jsonify({
                "action": "TRAIN", 
                "round": current_round, 
                "data": { "weights": weights_b64, "config": training_config }
            })

        return jsonify({"action": "WAIT"})


@app.route('/submit-weights', methods=['POST'])
def submit_weights():
    # ... (This function is unchanged) ...
    with SERVER_STATE_LOCK:
        client_id = request.json.get('client_id')
        if client_id not in SERVER_STATE["clients_in_round"]:
            return jsonify({"status": "error", "message": "Round already aggregated."}), 400

        weights_b64 = request.json.get('weights')
        metrics = request.json.get('metrics', {})
        
        weights_bytes = base64.b64decode(weights_b64)
        weights_array = np.frombuffer(weights_bytes, dtype=np.float32)

        num_samples = metrics.get('num_samples', 1)
        SERVER_STATE["client_updates_in_round"].append((weights_array, num_samples))
        
        pre_loss = metrics.get('pre_eval_loss', 0.0)
        pre_acc = metrics.get('pre_eval_accuracy', 0.0) * 100
        post_loss = metrics.get('post_eval_loss', 0.0)
        post_acc = metrics.get('post_eval_accuracy', 0.0) * 100
        train_time = metrics.get('training_time_ms', 'N/A')

        print(f"[{time.ctime()}] Received submission from '{client_id}':")
        print(f"  > Samples: {num_samples}, Train Time: {train_time}ms")
        print(f"  > Pre-Train Eval -> Loss: {pre_loss:.4f}, Accuracy: {pre_acc:.2f}%")
        print(f"  > Post-Train Eval -> Loss: {post_loss:.4f}, Accuracy: {post_acc:.2f}%")
        print(f"  > Submissions this round: {len(SERVER_STATE['client_updates_in_round'])}/{NUM_EXPECTED_CLIENTS}")

        if len(SERVER_STATE["client_updates_in_round"]) >= NUM_EXPECTED_CLIENTS:
            SERVER_STATE["server_status"] = "AGGREGATING"
            print(f"âœ… All submissions received for Round {SERVER_STATE['current_round']}. Aggregating...")
            
            total_samples = sum(item[1] for item in SERVER_STATE["client_updates_in_round"])
            if total_samples == 0: total_samples = 1
            weighted_updates = [item[0] * (item[1] / total_samples) for item in SERVER_STATE["client_updates_in_round"]]
            new_global_weights = np.sum(weighted_updates, axis=0)
            SERVER_STATE["global_model_weights"] = new_global_weights
            print(" > Global model updated via Federated Averaging.")
            
            SERVER_STATE["current_round"] += 1
            SERVER_STATE["clients_in_round"].clear()
            SERVER_STATE["client_updates_in_round"].clear()
            if SERVER_STATE["current_round"] > NUM_ROUNDS:
                SERVER_STATE["server_status"] = "COMPLETE"
                print("ðŸ All federated rounds are complete!")
                np.save(SCRIPT_DIR / "final_full_state_model.npy", new_global_weights)
                print(f"âœ… Final model state saved to '{SCRIPT_DIR / 'final_full_state_model.npy'}'")
            else:
                SERVER_STATE["server_status"] = "WAITING_FOR_CLIENTS"
                print(f"Server now WAITING for clients to join Round {SERVER_STATE['current_round']}.")
        return jsonify({"status": "success"})


@app.route('/reset_demo', methods=['POST'])
def reset_demo():
    # ... (This function is unchanged) ...
    print("\n" + "="*50 + "\n!!! RECEIVED REQUEST TO RESET SERVER STATE FOR DEMO !!!\n" + "="*50 + "\n")
    if load_initial_weights(is_reset=True):
        return jsonify({"message": "Server state reset successfully."}), 200
    else:
        return jsonify({"message": "Failed to reset server state."}), 500

if __name__ == '__main__':
    if load_initial_weights():
        print("\n--- FL Experiment Configuration ---")
        print(f" > Total Rounds:         {NUM_ROUNDS}")
        print(f" > Clients per Round:    {NUM_EXPECTED_CLIENTS}")
        print(f" > Local Epochs:         {LOCAL_EPOCHS}")
        print(f"-----------------------------------")
        print(f"\n--- Custom FL Server starting on http://{HOST}:{PORT} ---")
        serve(app, host=HOST, port=PORT)